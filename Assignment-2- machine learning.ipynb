{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41c4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae761ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38daefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1+ np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1df443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, n_hidden, learning_rate, n_iter):\n",
    "    m, n_input = X.shape\n",
    "    W1=np.random.randn(n_input, n_hidden)\n",
    "    b1=np.zeros((1, n_hidden))\n",
    "    W2=np.random.randn(n_hidden, 1)\n",
    "    b2=np.zeros((1,1))\n",
    "    for i in range(1, n_iter+1):\n",
    "        Z2=np.matmul(X,W1) + b1\n",
    "        A2=sigmoid(Z2)\n",
    "        Z3=np.matmul(A2, W2) + b2\n",
    "        A3=Z3\n",
    "        dZ3=A3-y\n",
    "        dW2=np.matmul(A2.T, dZ3)\n",
    "        db2=np.sum(dZ3, axis=0, keepdims=True)\n",
    "        dZ2=np.matmul(dZ3, W2.T) * sigmoid_derivative(Z2)\n",
    "        dW1=np.matmul(X.T, dZ2)\n",
    "        db1=np.sum(dZ2, axis=0)\n",
    "        W2=W2-learning_rate*dW2/m\n",
    "        b2=b2-learning_rate*db2/m\n",
    "        W1=W1-learning_rate*dW1/m\n",
    "        \n",
    "        b1=b1-learning_rate*db1/m\n",
    "        if i % 100==0:\n",
    "            cost=np.mean((y-A3)**2)\n",
    "            print('Iteration %i, training loss: %f' % (i,cost))\n",
    "    model={'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5ff0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Boston data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12cb48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "boston = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([boston.values[::2, :], boston.values[1::2, :2]])\n",
    "target = boston.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82239dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boston=datasets.load_boston()\n",
    "num_test=10 #the last 10 samples as testing set\n",
    "from sklearn import preprocessing\n",
    "scaler=preprocessing.StandardScaler()\n",
    "X_train=data[:-num_test, :]\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "y_train=target[:-num_test].reshape(-1,1)\n",
    "X_test=data[-num_test:, :]\n",
    "X_test=scaler.transform(X_test)\n",
    "y_test=target[-num_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0a0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, training loss: 13.028557\n",
      "Iteration 200, training loss: 9.046222\n",
      "Iteration 300, training loss: 7.360868\n",
      "Iteration 400, training loss: 6.418070\n",
      "Iteration 500, training loss: 5.819625\n",
      "Iteration 600, training loss: 5.378253\n",
      "Iteration 700, training loss: 4.992037\n",
      "Iteration 800, training loss: 4.653372\n",
      "Iteration 900, training loss: 4.388898\n",
      "Iteration 1000, training loss: 4.171886\n",
      "Iteration 1100, training loss: 3.980584\n",
      "Iteration 1200, training loss: 3.806860\n",
      "Iteration 1300, training loss: 3.649822\n",
      "Iteration 1400, training loss: 3.507116\n",
      "Iteration 1500, training loss: 3.375375\n",
      "Iteration 1600, training loss: 3.253883\n",
      "Iteration 1700, training loss: 3.143197\n",
      "Iteration 1800, training loss: 3.041324\n",
      "Iteration 1900, training loss: 2.948273\n",
      "Iteration 2000, training loss: 2.865341\n"
     ]
    }
   ],
   "source": [
    "n_hidden=20\n",
    "learning_rate=0.1\n",
    "n_iter=2000\n",
    "model=train(X_train,y_train,n_hidden,learning_rate,n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe04aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    W1=model['W1']\n",
    "    b1=model['b1']\n",
    "    W2=model['W2']\n",
    "    b2=model['b2']\n",
    "    A2=sigmoid(np.matmul(x,W1)+b1)\n",
    "    A3=np.matmul(A2, W2)+b2\n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3168d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=predict(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f339ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19.56704676]\n",
      " [19.3796713 ]\n",
      " [21.30793505]\n",
      " [20.26612855]\n",
      " [19.38243945]\n",
      " [29.07565355]\n",
      " [26.50946734]\n",
      " [30.74275685]\n",
      " [28.56865957]\n",
      " [25.11370734]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969c7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36422a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementating scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1bfe88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "nn_scikit=MLPRegressor(hidden_layer_sizes=(16,8), activation='relu', solver='adam', \n",
    "                       learning_rate_init=0.001, random_state=42, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51555ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1599: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.79582331 18.55538023 21.07961496 19.21362606 18.50955771 23.5608387\n",
      " 22.27916529 27.11909153 24.70251262 22.05522035]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nn_scikit.fit(X_train, y_train)\n",
    "predictions_1=nn_scikit.predict(X_test)\n",
    "print(predictions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838942d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.933482332708786\n"
     ]
    }
   ],
   "source": [
    "print(np.mean((y_test-predictions_1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bd95c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing neural networks with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "048d668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.14.0-cp39-cp39-macosx_10_15_x86_64.whl (229.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 229.6 MB 31.4 MB/s eta 0:00:01    |███████████▋                    | 83.0 MB 51.6 MB/s eta 0:00:03     |█████████████████▌              | 125.3 MB 31.8 MB/s eta 0:00:04     |█████████████████████████▎      | 181.0 MB 1.0 MB/s eta 0:00:47\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 27.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 38.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 30.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 4.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting ml-dtypes==0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorboard<2.15,>=2.14\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.24.3-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 47.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[K     |████████████████████████████████| 440 kB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.23.5\n",
      "  Downloading numpy-1.26.0-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 34.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras<2.15,>=2.14.0\n",
      "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 32.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.3.4)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.33.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.0-cp39-cp39-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 30.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.8)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 35.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vimalrajmatmari/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Installing collected packages: google-auth, tensorboard-data-server, protobuf, numpy, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, ml-dtypes, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.33.0\n",
      "    Uninstalling google-auth-1.33.0:\n",
      "      Successfully uninstalled google-auth-1.33.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.5\n",
      "    Uninstalling numpy-1.21.5:\n",
      "      Successfully uninstalled numpy-1.21.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.26.0 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.26.0 which is incompatible.\n",
      "google-cloud-storage 1.31.0 requires google-auth<2.0dev,>=1.11.0, but you have google-auth 2.23.2 which is incompatible.\n",
      "google-cloud-core 1.7.1 requires google-auth<2.0dev,>=1.24.0, but you have google-auth 2.23.2 which is incompatible.\n",
      "google-api-core 1.25.1 requires google-auth<2.0dev,>=1.21.1, but you have google-auth 2.23.2 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.2 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 keras-2.14.0 libclang-16.0.6 ml-dtypes-0.2.0 numpy-1.26.0 opt-einsum-3.3.0 protobuf-4.24.3 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f1219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 11:51:31.878049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb14c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential([keras.layers.Dense(units=20, activation='relu'), keras.layers.Dense(units=8, activation='relu'),keras.layers.Dense(units=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67796985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50db0f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 2ms/step - loss: 413.3131\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 92.6940\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 33.4044\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 23.7314\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 18.8016\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 16.7342\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 14.2046\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 14.3343\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 12.6288\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 11.4486\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 11.3208\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 10.8952\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 10.8978\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 10.6525\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.8892\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.3999\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.5888\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.3419\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.2982\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.4026\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.5964\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.1926\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.7009\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.9177\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.9857\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.6503\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.2041\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 10.1788\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.8376\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.4558\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.1545\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.2014\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.0080\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.1497\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.5381\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.7103\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.1646\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.9633\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.5594\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.0052\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.0900\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.9768\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.1418\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.8572\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.0994\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.1248\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.1230\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.8314\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.8334\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.1914\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.8470\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4906\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.5360\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.8912\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.3826\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.5257\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.9479\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4346\n",
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.1088\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.9817\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.6692\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.0393\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.2934\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.3158\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.2970\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.9532\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.6783\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.5338\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.9562\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.6867\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.1080\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.0880\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.6067\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.6330\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.7677\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.1305\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.9762\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.3582\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.2636\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.3379\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.8040\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.5650\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.1542\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.3443\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.3824\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.1112\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.4524\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.8465\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.7786\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.9107\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.6111\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4858\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4900\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.3662\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.6616\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.3185\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.0701\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.7296\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.2714\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.8554\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.7637\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 5.7388\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.9530\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.3018\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.4107\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.3143\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.7392\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.4197\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3575\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.5402\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.9293\n",
      "Epoch 112/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3809\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3624\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.8647\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.6747\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.9476\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.1469\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.5579\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.5620\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.8306\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.6722\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.4097\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3968\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0108\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3816\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0873\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.2175\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3832\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3257\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.9212\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.6867\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.4599\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.8253\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.9581\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.9165\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.6581\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.5476\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.2376\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.7650\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3489\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.7502\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.2488\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.8956\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6348\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6721\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.8636\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6264\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.3840\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.7656\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.9322\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.4571\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5174\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.7895\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.2001\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.1351\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0601\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6375\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5356\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.1256\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.9544\n",
      "Epoch 161/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0000\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5375\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5604\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1560\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2745\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5002\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.7217\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5210\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3215\n",
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1715\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3728\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1203\n",
      "Epoch 173/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2002\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2890\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.4527\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.8584\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.8225\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3988\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.2257\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0306\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0707\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1092\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6468\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.1917\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.9993\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5666\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8864\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.9252\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2328\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0386\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1772\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1818\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7345\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.9365\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.1626\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1985\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3343\n",
      "Epoch 198/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0124\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2101\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3105\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2517\n",
      "Epoch 202/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0181\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0990\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3387\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3564\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0085\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7105\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5776\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8800\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7313\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5732\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5796\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0996\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5469\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8393\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5247\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.6082\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3518\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3377\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7257\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1984\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3137\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.9615\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.3169\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.4803\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.5083\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5002\n",
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.4931\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5515\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8955\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5479\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.9884\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8829\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0076\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5030\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2137\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6523\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8809\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3890\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4949\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4385\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5981\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5828\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.6322\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1709\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1131\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7511\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2530\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2782\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7435\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3090\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3036\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4553\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.6405\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5469\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7427\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5959\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8213\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.6472\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.4142\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4821\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3331\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4097\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3224\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.9867\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0290\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1733\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5692\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3949\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4083\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.6971\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7280\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3919\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2408\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3290\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1381\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4671\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2568\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.9271\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3412\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2373\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.7115\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.9632\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.5836\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.0976\n",
      "Epoch 286/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4407\n",
      "Epoch 287/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1606\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.8766\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2729\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5835\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2852\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4024\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1182\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1855\n",
      "Epoch 295/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8417\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2584\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3164\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.4248\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2098\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe3d3dacc70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7993bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n",
      "[20.322912 20.182194 22.984016 20.029615 20.895344 24.403944 21.615715\n",
      " 24.739471 23.47913  20.743153]\n"
     ]
    }
   ],
   "source": [
    "predictions_2=model.predict(X_test)[:,0]\n",
    "print(predictions_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98301421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.642553640569602\n"
     ]
    }
   ],
   "source": [
    "print(np.mean((y_test-predictions_2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b14509df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preventing overfitting in neural networks\n",
    "model=keras.Sequential([keras.layers.Dense(units=32, activation='relu'), tf.keras.layers.Dropout(0.5),keras.layers.Dense(units=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b004a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting stock prices with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aa4c4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-12-01</th>\n",
       "      <td>10806.0</td>\n",
       "      <td>10934.9</td>\n",
       "      <td>10806.0</td>\n",
       "      <td>10912.6</td>\n",
       "      <td>256932865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-02</th>\n",
       "      <td>10912.0</td>\n",
       "      <td>10921.4</td>\n",
       "      <td>10861.7</td>\n",
       "      <td>10877.5</td>\n",
       "      <td>214888854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-05</th>\n",
       "      <td>10877.0</td>\n",
       "      <td>10877.0</td>\n",
       "      <td>10810.7</td>\n",
       "      <td>10835.0</td>\n",
       "      <td>237430947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-06</th>\n",
       "      <td>10835.4</td>\n",
       "      <td>10936.2</td>\n",
       "      <td>10835.4</td>\n",
       "      <td>10856.9</td>\n",
       "      <td>264721465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-07</th>\n",
       "      <td>10856.9</td>\n",
       "      <td>10868.1</td>\n",
       "      <td>10764.0</td>\n",
       "      <td>10810.9</td>\n",
       "      <td>243543206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-08</th>\n",
       "      <td>10808.4</td>\n",
       "      <td>10847.2</td>\n",
       "      <td>10729.7</td>\n",
       "      <td>10755.1</td>\n",
       "      <td>253313750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-09</th>\n",
       "      <td>10751.8</td>\n",
       "      <td>10806.0</td>\n",
       "      <td>10729.9</td>\n",
       "      <td>10778.6</td>\n",
       "      <td>238907145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open     High      Low    Close     Volume\n",
       "Date                                                     \n",
       "2005-12-01  10806.0  10934.9  10806.0  10912.6  256932865\n",
       "2005-12-02  10912.0  10921.4  10861.7  10877.5  214888854\n",
       "2005-12-05  10877.0  10877.0  10810.7  10835.0  237430947\n",
       "2005-12-06  10835.4  10936.2  10835.4  10856.9  264721465\n",
       "2005-12-07  10856.9  10868.1  10764.0  10810.9  243543206\n",
       "2005-12-08  10808.4  10847.2  10729.7  10755.1  253313750\n",
       "2005-12-09  10751.8  10806.0  10729.9  10778.6  238907145"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata=pd.read_csv(r\"/Users/vimalrajmatmari/Documents/Machine learning/Dow Jones Industrial Average Prices/20051201_20051210.csv\",index_col='Date')\n",
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71c12380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement feature generation by starting with a sub-function that directly creates features from the original six features\n",
    "def add_original_feature(df, df_new):\n",
    "    df_new['open'] = df['Open']\n",
    "    df_new['open_1'] = df['Open'].shift(1)\n",
    "    df_new['close_1'] = df['Close'].shift(1)\n",
    "    df_new['high_1'] = df['High'].shift(1)\n",
    "    df_new['low_1'] = df['Low'].shift(1)\n",
    "    df_new['volume_1'] = df['Volume'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6a4ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#develop a sub-function that generates six features related to average close prices\n",
    "def add_avg_price(df, df_new):\n",
    "    df_new['avg_price_5']=df['Close'].rolling(5).mean().shift(1)\n",
    "    df_new['avg_price_30']=df['Close'].rolling(21).mean().shift(1)\n",
    "    df_new['avg_price_365']=df['Close'].rolling(252).mean().shift(1)\n",
    "    df_new['ratio_avg_price_5_30']=df_new['avg_price_5']/df_new['avg_price_30']\n",
    "    df_new['ratio_avg_price_5_365']=df_new['avg_price_5']/df_new['avg_price_365']\n",
    "    df_new['ratio_avg_price_30_365']=df_new['avg_price_30']/df_new['avg_price_365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d9254d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a sub-function that generates six features related to average volumes is as follows\n",
    "def add_avg_volume(df, df_new):\n",
    "    df_new['avg_volume_5'] = df['Volume'].rolling(5).mean().shift(1)\n",
    "    df_new['avg_volume_30'] =df['Volume'].rolling(21).mean().shift(1)\n",
    "    df_new['avg_volume_365'] =df['Volume'].rolling(252).mean().shift(1)\n",
    "    df_new['ratio_avg_volume_5_30']=df_new['avg_volume_5']/df_new['avg_volume_30']\n",
    "    df_new['ratio_avg_volume_5_365']=df_new['avg_volume_5']/df_new['avg_volume_365']\n",
    "    df_new['ratio_avg_volume_30_365']=df_new['avg_volume_30']/df_new['avg_volume_365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fabaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the standard deviation, we develop the following sub-function for the price related features\n",
    "def add_std_price(df, df_new):\n",
    "    df_new['std_price_5'] =df['Close'].rolling(5).std().shift(1)\n",
    "    df_new['std_price_30'] =df['Close'].rolling(21).std().shift(1)\n",
    "    df_new['std_price_365'] =df['Close'].rolling(252).std().shift(1)\n",
    "    df_new['ratio_std_price_5_30'] =df_new['std_price_5']/df_new['std_price_30']\n",
    "    df_new['ratio_std_price_5_365']=df_new['std_price_5']/df_new['std_price_365']\n",
    "    df_new['ratio_std_price_30_365']=df_new['std_price_30']/df_new['std_price_365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be3aeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, a sub-function that generates six volume-based standard deviation features is as follows.\n",
    "def add_std_volume(df, df_new):\n",
    "    df_new['std_volume_5']=df['Volume'].rolling(5).std().shift(1)\n",
    "    df_new['std_volume_30']=df['Volume'].rolling(21).std().shift(1)\n",
    "    df_new['std_volume_365'] =df['Volume'].rolling(252).std().shift (1)\n",
    "    df_new['ratio_std_volume_5_30']=df_new['std_volume_5']/df_new['std_volume_30']\n",
    "    df_new['ratio_std_volume_5_365'] =df_new['std_volume_5']/df_new['std_volume_365']\n",
    "    df_new['ratio_std_volume_30_365'] =df_new['std_volume_30']/df_new['std_volume_365']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffa37887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seven return-based features are generated using the following sub-function.\n",
    "def add_return_feature(df, df_new):\n",
    "    df_new['return_1'] = ((df['Close'] - df['Close'].shift(1))/df['Close'].shift(1)).shift(1)\n",
    "    df_new['return_5'] = ((df['Close'] - df['Close'].shift(5))/df['Close'].shift(5)).shift(1)\n",
    "    df_new['return_30'] = ((df['Close'] -df['Close'].shift(21))/df['Close'].shift (21)).shift(1)\n",
    "    df_new['return_365'] = ((df['Close'] -df['Close'].shift(252))/df['Close'].shift (252)).shift(1)\n",
    "    df_new['moving_avg_5'] =df_new['return_1'].rolling(5).mean().shift(1)\n",
    "    df_new['moving_avg_30'] =df_new['return_1'].rolling(21).mean().shift(1)\n",
    "    df_new['moving_avg_365'] =df_new['return_1'].rolling(252).mean().shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adaf6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we put together the main feature generation function that calls all the preceding sub-functions.\n",
    "def generate_features(df):\n",
    "    \n",
    "    df_new=pd.DataFrame()\n",
    "    # 6 original features\n",
    "    add_original_feature(df, df_new)\n",
    "    # 31 generated features\n",
    "    add_avg_price(df,df_new)\n",
    "    add_avg_volume(df, df_new)\n",
    "    add_std_price(df, df_new)\n",
    "    add_std_volume(df,df_new)\n",
    "    add_return_feature(df,df_new)\n",
    "    #the target\n",
    "    df_new['close']=df['Close']\n",
    "    df_new=df_new.dropna(axis=0)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2bb2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw=pd.read_csv(r\"/Users/vimalrajmatmari/Documents/Machine learning/Dow Jones Industrial Average Prices/19880101_20191231.csv\", index_col='Date')\n",
    "data=generate_features(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1903dcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              open  open_1  close_1  high_1   low_1    volume_1  avg_price_5  \\\n",
      "Date                                                                           \n",
      "1989-01-04  2146.6  2168.4   2144.6  2168.4  2127.1  17302883.0      2165.00   \n",
      "1989-01-05  2177.7  2146.6   2177.7  2183.4  2146.6  15714720.0      2168.00   \n",
      "1989-01-06  2190.5  2177.7   2190.5  2205.2  2173.0  20303094.0      2172.82   \n",
      "1989-01-09  2194.3  2190.5   2194.3  2213.8  2182.3  16494441.0      2175.14   \n",
      "1989-01-10  2199.5  2194.3   2199.5  2209.1  2185.0  18410324.0      2181.32   \n",
      "\n",
      "            avg_price_30  avg_price_365  ratio_avg_price_5_30  ...  \\\n",
      "Date                                                           ...   \n",
      "1989-01-04      2150.624       2062.113                 1.007  ...   \n",
      "1989-01-05      2154.690       2062.668                 1.006  ...   \n",
      "1989-01-06      2157.867       2063.218                 1.007  ...   \n",
      "1989-01-09      2160.005       2064.341                 1.007  ...   \n",
      "1989-01-10      2162.190       2065.351                 1.009  ...   \n",
      "\n",
      "            ratio_std_volume_5_365  ratio_std_volume_30_365  return_1  \\\n",
      "Date                                                                    \n",
      "1989-01-04                   0.563                    0.723    -0.011   \n",
      "1989-01-05                   0.474                    0.724     0.015   \n",
      "1989-01-06                   0.580                    0.748     0.006   \n",
      "1989-01-09                   0.516                    0.746     0.002   \n",
      "1989-01-10                   0.279                    0.742     0.002   \n",
      "\n",
      "            return_5  return_30  return_365  moving_avg_5  moving_avg_30  \\\n",
      "Date                                                                       \n",
      "1989-01-04    -0.011      0.020       0.056         0.001          0.001   \n",
      "1989-01-05     0.007      0.041       0.069        -0.002          0.001   \n",
      "1989-01-06     0.011      0.031       0.068         0.001          0.002   \n",
      "1989-01-09     0.005      0.021       0.148         0.002          0.001   \n",
      "1989-01-10     0.014      0.021       0.131         0.001          0.001   \n",
      "\n",
      "            moving_avg_365   close  \n",
      "Date                                \n",
      "1989-01-04           0.000  2177.7  \n",
      "1989-01-05           0.000  2190.5  \n",
      "1989-01-06           0.000  2194.3  \n",
      "1989-01-09           0.000  2199.5  \n",
      "1989-01-10           0.001  2193.2  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data.round(decimals=3).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2b4e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load the stock data, generate features, and label the generate_features function, Predicting Stock Prices with Regression Algorithms:\n",
    "data_raw=pd.read_csv(r\"/Users/vimalrajmatmari/Documents/Machine learning/Dow Jones Industrial Average Prices/19880101_20191231.csv\", index_col='Date')\n",
    "data=generate_features(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a9282e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We construct the training set using data from 1988 to 2018 and the testing set using data from 2019:\n",
    "start_train='1988-01-01'\n",
    "end_train='2018-12-31'\n",
    "start_test='2019-01-01'\n",
    "end_test='2019-12-31'\n",
    "data_train=data.loc[start_train:end_train]\n",
    "X_train=data_train.drop('close', axis=1).values\n",
    "y_train=data_train['close'].values\n",
    "data_test=data.loc[start_test:end_test]\n",
    "X_test=data_test.drop('close',axis=1).values\n",
    "y_test=data_test['close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bd6894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to normalize features into the same or a comparable scale. We do so by removing the mean and rescaling to unit variance:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9161c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We rescale both sets with the scaler taught by the training set:\n",
    "X_scaled_train=scaler.fit_transform(X_train)\n",
    "X_scaled_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36f8a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now build a neural network model using the Keras Sequential API:\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model=Sequential([Dense(units=32, activation='relu'), Dense(units=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9518da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we compile the model by using Adam as the optimizer with a learning rate of 0.1 and MSE as the learning goal:\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cccec528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "237/237 [==============================] - 1s 1ms/step - loss: 33826648.0000\n",
      "Epoch 2/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 2016950.7500\n",
      "Epoch 3/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 490370.3438\n",
      "Epoch 4/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 157370.5000\n",
      "Epoch 5/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 69509.6953\n",
      "Epoch 6/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 41305.3164\n",
      "Epoch 7/100\n",
      "237/237 [==============================] - 0s 944us/step - loss: 29538.2754\n",
      "Epoch 8/100\n",
      "237/237 [==============================] - 0s 913us/step - loss: 25205.7578\n",
      "Epoch 9/100\n",
      "237/237 [==============================] - 0s 911us/step - loss: 25825.3828\n",
      "Epoch 10/100\n",
      "237/237 [==============================] - 0s 940us/step - loss: 23978.1289\n",
      "Epoch 11/100\n",
      "237/237 [==============================] - 0s 908us/step - loss: 28834.5703\n",
      "Epoch 12/100\n",
      "237/237 [==============================] - 0s 910us/step - loss: 23295.0117\n",
      "Epoch 13/100\n",
      "237/237 [==============================] - 0s 928us/step - loss: 24532.0234\n",
      "Epoch 14/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 28005.3711\n",
      "Epoch 15/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 28685.2500\n",
      "Epoch 16/100\n",
      "237/237 [==============================] - 0s 906us/step - loss: 25805.1836\n",
      "Epoch 17/100\n",
      "237/237 [==============================] - 0s 913us/step - loss: 30377.9004\n",
      "Epoch 18/100\n",
      "237/237 [==============================] - 0s 922us/step - loss: 29647.9785\n",
      "Epoch 19/100\n",
      "237/237 [==============================] - 0s 985us/step - loss: 32791.8320\n",
      "Epoch 20/100\n",
      "237/237 [==============================] - 0s 981us/step - loss: 29843.6504\n",
      "Epoch 21/100\n",
      "237/237 [==============================] - 0s 968us/step - loss: 31348.7422\n",
      "Epoch 22/100\n",
      "237/237 [==============================] - 0s 916us/step - loss: 29003.5938\n",
      "Epoch 23/100\n",
      "237/237 [==============================] - 0s 948us/step - loss: 33039.0430\n",
      "Epoch 24/100\n",
      "237/237 [==============================] - 0s 921us/step - loss: 26072.5488\n",
      "Epoch 25/100\n",
      "237/237 [==============================] - 0s 966us/step - loss: 29603.1484\n",
      "Epoch 26/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 29609.0742\n",
      "Epoch 27/100\n",
      "237/237 [==============================] - 0s 992us/step - loss: 27634.9883\n",
      "Epoch 28/100\n",
      "237/237 [==============================] - 0s 920us/step - loss: 29611.3965\n",
      "Epoch 29/100\n",
      "237/237 [==============================] - 0s 991us/step - loss: 27393.4883\n",
      "Epoch 30/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 37005.7148\n",
      "Epoch 31/100\n",
      "237/237 [==============================] - 0s 935us/step - loss: 25241.4727\n",
      "Epoch 32/100\n",
      "237/237 [==============================] - 0s 917us/step - loss: 25138.0664\n",
      "Epoch 33/100\n",
      "237/237 [==============================] - 0s 977us/step - loss: 28694.7637\n",
      "Epoch 34/100\n",
      "237/237 [==============================] - 0s 968us/step - loss: 27227.2871\n",
      "Epoch 35/100\n",
      "237/237 [==============================] - 0s 935us/step - loss: 24281.7422\n",
      "Epoch 36/100\n",
      "237/237 [==============================] - 0s 976us/step - loss: 26424.0938\n",
      "Epoch 37/100\n",
      "237/237 [==============================] - 0s 977us/step - loss: 28935.2949\n",
      "Epoch 38/100\n",
      "237/237 [==============================] - 0s 946us/step - loss: 26417.7676\n",
      "Epoch 39/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 29398.2227\n",
      "Epoch 40/100\n",
      "237/237 [==============================] - 0s 978us/step - loss: 24616.8613\n",
      "Epoch 41/100\n",
      "237/237 [==============================] - 0s 956us/step - loss: 23778.4453\n",
      "Epoch 42/100\n",
      "237/237 [==============================] - 0s 930us/step - loss: 27674.1270\n",
      "Epoch 43/100\n",
      "237/237 [==============================] - 0s 956us/step - loss: 21629.5059\n",
      "Epoch 44/100\n",
      "237/237 [==============================] - 0s 961us/step - loss: 26329.8320\n",
      "Epoch 45/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 23712.5039\n",
      "Epoch 46/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 31195.5000\n",
      "Epoch 47/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 24357.5879\n",
      "Epoch 48/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 22929.9121\n",
      "Epoch 49/100\n",
      "237/237 [==============================] - 1s 2ms/step - loss: 26749.2461\n",
      "Epoch 50/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 24320.4570\n",
      "Epoch 51/100\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 24558.0391\n",
      "Epoch 52/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 24741.9316\n",
      "Epoch 53/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 23172.9941\n",
      "Epoch 54/100\n",
      "237/237 [==============================] - 0s 950us/step - loss: 24974.1172\n",
      "Epoch 55/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 23777.6582\n",
      "Epoch 56/100\n",
      "237/237 [==============================] - 0s 2ms/step - loss: 22380.5879\n",
      "Epoch 57/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 25883.4805\n",
      "Epoch 58/100\n",
      "237/237 [==============================] - 0s 977us/step - loss: 26092.3320\n",
      "Epoch 59/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 24645.0508\n",
      "Epoch 60/100\n",
      "237/237 [==============================] - 0s 935us/step - loss: 23315.5117\n",
      "Epoch 61/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 22798.2344\n",
      "Epoch 62/100\n",
      "237/237 [==============================] - 0s 964us/step - loss: 20939.0605\n",
      "Epoch 63/100\n",
      "237/237 [==============================] - 0s 937us/step - loss: 21613.2559\n",
      "Epoch 64/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 23450.4121\n",
      "Epoch 65/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 22044.4004\n",
      "Epoch 66/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 23702.9551\n",
      "Epoch 67/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 23256.9414\n",
      "Epoch 68/100\n",
      "237/237 [==============================] - 0s 945us/step - loss: 28001.5410\n",
      "Epoch 69/100\n",
      "237/237 [==============================] - 0s 928us/step - loss: 19862.6953\n",
      "Epoch 70/100\n",
      "237/237 [==============================] - 0s 970us/step - loss: 21985.6230\n",
      "Epoch 71/100\n",
      "237/237 [==============================] - 0s 959us/step - loss: 28456.5840\n",
      "Epoch 72/100\n",
      "237/237 [==============================] - 0s 937us/step - loss: 26849.3281\n",
      "Epoch 73/100\n",
      "237/237 [==============================] - 0s 925us/step - loss: 21564.9434\n",
      "Epoch 74/100\n",
      "237/237 [==============================] - 0s 917us/step - loss: 21584.1699\n",
      "Epoch 75/100\n",
      "237/237 [==============================] - 0s 973us/step - loss: 20411.4258\n",
      "Epoch 76/100\n",
      "237/237 [==============================] - 0s 930us/step - loss: 19563.0449\n",
      "Epoch 77/100\n",
      "237/237 [==============================] - 0s 915us/step - loss: 22350.7949\n",
      "Epoch 78/100\n",
      "237/237 [==============================] - 0s 914us/step - loss: 23745.7832\n",
      "Epoch 79/100\n",
      "237/237 [==============================] - 0s 927us/step - loss: 21268.9414\n",
      "Epoch 80/100\n",
      "237/237 [==============================] - 0s 964us/step - loss: 21455.1738\n",
      "Epoch 81/100\n",
      "237/237 [==============================] - 0s 974us/step - loss: 22644.0234\n",
      "Epoch 82/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 20533.5371\n",
      "Epoch 83/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 20953.8008\n",
      "Epoch 84/100\n",
      "237/237 [==============================] - 0s 997us/step - loss: 24129.1836\n",
      "Epoch 85/100\n",
      "237/237 [==============================] - 0s 913us/step - loss: 21643.5312\n",
      "Epoch 86/100\n",
      "237/237 [==============================] - 0s 1000us/step - loss: 21161.5469\n",
      "Epoch 87/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 21889.4785\n",
      "Epoch 88/100\n",
      "237/237 [==============================] - 0s 924us/step - loss: 24051.2188\n",
      "Epoch 89/100\n",
      "237/237 [==============================] - 0s 954us/step - loss: 22011.6875\n",
      "Epoch 90/100\n",
      "237/237 [==============================] - 0s 981us/step - loss: 21271.9355\n",
      "Epoch 91/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 22446.0430\n",
      "Epoch 92/100\n",
      "237/237 [==============================] - 0s 953us/step - loss: 22063.4844\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 0s 1ms/step - loss: 19769.0078\n",
      "Epoch 94/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 24970.9121\n",
      "Epoch 95/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 21735.2656\n",
      "Epoch 96/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 19469.3066\n",
      "Epoch 97/100\n",
      "237/237 [==============================] - 0s 1ms/step - loss: 21534.7852\n",
      "Epoch 98/100\n",
      "237/237 [==============================] - 0s 898us/step - loss: 20238.2344\n",
      "Epoch 99/100\n",
      "237/237 [==============================] - 0s 906us/step - loss: 19667.6348\n",
      "Epoch 100/100\n",
      "237/237 [==============================] - 0s 886us/step - loss: 21117.0586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe3d4283400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After defining the model, we now train it against the training set:\n",
    "model.fit(X_scaled_train, y_train, epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4eea647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "[[23580.11 ]\n",
      " [23591.193]\n",
      " [23085.87 ]\n",
      " [23756.422]\n",
      " [23859.4  ]\n",
      " [24095.861]\n",
      " [24133.498]\n",
      " [24262.42 ]\n",
      " [24252.807]\n",
      " [24212.822]\n",
      " [24391.607]\n",
      " [24505.63 ]\n",
      " [24717.596]\n",
      " [24966.629]\n",
      " [24746.357]\n",
      " [24910.332]\n",
      " [24896.068]\n",
      " [25016.04 ]\n",
      " [24784.236]\n",
      " [24969.176]\n",
      " [25307.9  ]\n",
      " [25207.918]\n",
      " [25351.318]\n",
      " [25538.916]\n",
      " [25701.38 ]\n",
      " [25626.684]\n",
      " [25452.447]\n",
      " [25438.344]\n",
      " [25461.627]\n",
      " [25800.527]\n",
      " [25890.736]\n",
      " [25812.615]\n",
      " [26214.203]\n",
      " [26248.436]\n",
      " [26298.16 ]\n",
      " [26226.541]\n",
      " [26436.693]\n",
      " [26492.623]\n",
      " [26426.842]\n",
      " [26357.701]\n",
      " [26308.926]\n",
      " [26432.906]\n",
      " [26199.822]\n",
      " [26191.674]\n",
      " [26069.861]\n",
      " [25827.74 ]\n",
      " [25725.643]\n",
      " [25924.084]\n",
      " [25937.135]\n",
      " [26049.479]\n",
      " [26067.467]\n",
      " [26014.277]\n",
      " [26172.182]\n",
      " [26172.285]\n",
      " [25993.236]\n",
      " [26152.922]\n",
      " [25883.666]\n",
      " [25947.166]\n",
      " [26078.16 ]\n",
      " [26042.926]\n",
      " [26089.725]\n",
      " [26367.256]\n",
      " [26675.354]\n",
      " [26609.451]\n",
      " [26644.74 ]\n",
      " [26788.781]\n",
      " [26796.51 ]\n",
      " [26684.35 ]\n",
      " [26572.15 ]\n",
      " [26615.945]\n",
      " [26644.627]\n",
      " [26772.818]\n",
      " [26769.375]\n",
      " [26831.918]\n",
      " [26787.152]\n",
      " [26878.713]\n",
      " [26869.293]\n",
      " [26988.74 ]\n",
      " [26910.713]\n",
      " [26795.006]\n",
      " [26901.064]\n",
      " [26962.123]\n",
      " [26946.768]\n",
      " [26832.947]\n",
      " [26736.38 ]\n",
      " [26812.217]\n",
      " [26694.65 ]\n",
      " [26375.037]\n",
      " [26403.443]\n",
      " [26193.06 ]\n",
      " [26181.17 ]\n",
      " [25685.363]\n",
      " [25922.014]\n",
      " [26011.807]\n",
      " [26194.127]\n",
      " [26094.65 ]\n",
      " [26089.604]\n",
      " [26265.486]\n",
      " [26138.629]\n",
      " [25861.8  ]\n",
      " [26009.729]\n",
      " [25715.299]\n",
      " [25466.03 ]\n",
      " [25500.97 ]\n",
      " [25197.186]\n",
      " [25209.299]\n",
      " [25714.838]\n",
      " [25906.021]\n",
      " [26095.74 ]\n",
      " [26421.943]\n",
      " [26525.33 ]\n",
      " [26466.604]\n",
      " [26397.129]\n",
      " [26479.31 ]\n",
      " [26469.564]\n",
      " [26540.703]\n",
      " [26844.78 ]\n",
      " [26937.982]\n",
      " [27131.072]\n",
      " [27016.23 ]\n",
      " [27088.963]\n",
      " [26936.26 ]\n",
      " [26911.893]\n",
      " [26916.607]\n",
      " [26956.135]\n",
      " [27083.584]\n",
      " [27117.342]\n",
      " [27292.486]\n",
      " [27193.87 ]\n",
      " [27191.912]\n",
      " [27220.705]\n",
      " [27346.854]\n",
      " [27515.75 ]\n",
      " [27754.08 ]\n",
      " [27807.135]\n",
      " [27781.416]\n",
      " [27671.598]\n",
      " [27656.143]\n",
      " [27593.787]\n",
      " [27610.023]\n",
      " [27717.605]\n",
      " [27651.832]\n",
      " [27570.73 ]\n",
      " [27625.344]\n",
      " [27629.156]\n",
      " [27570.502]\n",
      " [27273.785]\n",
      " [27044.533]\n",
      " [26812.148]\n",
      " [26153.992]\n",
      " [26390.74 ]\n",
      " [26312.264]\n",
      " [26720.143]\n",
      " [26603.76 ]\n",
      " [26333.127]\n",
      " [26567.805]\n",
      " [25909.117]\n",
      " [25989.453]\n",
      " [26316.406]\n",
      " [26542.29 ]\n",
      " [26411.793]\n",
      " [26668.998]\n",
      " [26642.152]\n",
      " [26092.748]\n",
      " [26339.258]\n",
      " [26172.943]\n",
      " [26416.238]\n",
      " [26764.217]\n",
      " [26355.936]\n",
      " [26465.943]\n",
      " [26699.203]\n",
      " [27076.701]\n",
      " [27164.314]\n",
      " [27319.787]\n",
      " [27343.533]\n",
      " [27597.967]\n",
      " [27691.146]\n",
      " [27691.03 ]\n",
      " [27558.342]\n",
      " [27604.098]\n",
      " [27653.502]\n",
      " [27652.035]\n",
      " [27314.389]\n",
      " [27404.697]\n",
      " [27260.688]\n",
      " [27390.24 ]\n",
      " [27337.244]\n",
      " [27349.22 ]\n",
      " [27437.865]\n",
      " [27007.686]\n",
      " [26512.502]\n",
      " [26619.652]\n",
      " [26969.268]\n",
      " [26821.182]\n",
      " [26607.338]\n",
      " [26798.197]\n",
      " [26972.783]\n",
      " [27223.4  ]\n",
      " [27213.58 ]\n",
      " [27424.572]\n",
      " [27425.035]\n",
      " [27442.377]\n",
      " [27209.229]\n",
      " [27243.623]\n",
      " [27221.93 ]\n",
      " [27245.379]\n",
      " [27182.562]\n",
      " [27340.453]\n",
      " [27483.986]\n",
      " [27476.87 ]\n",
      " [27571.617]\n",
      " [27467.104]\n",
      " [27756.143]\n",
      " [27887.5  ]\n",
      " [27902.516]\n",
      " [27932.082]\n",
      " [28108.62 ]\n",
      " [28060.01 ]\n",
      " [28085.455]\n",
      " [28097.49 ]\n",
      " [28152.557]\n",
      " [28192.09 ]\n",
      " [28409.402]\n",
      " [28469.332]\n",
      " [28370.525]\n",
      " [28234.273]\n",
      " [28235.365]\n",
      " [28341.385]\n",
      " [28497.832]\n",
      " [28554.736]\n",
      " [28599.209]\n",
      " [28543.232]\n",
      " [28198.22 ]\n",
      " [27981.342]\n",
      " [28217.67 ]\n",
      " [28203.281]\n",
      " [28481.176]\n",
      " [28396.678]\n",
      " [28370.979]\n",
      " [28377.24 ]\n",
      " [28556.861]\n",
      " [28609.54 ]\n",
      " [28691.834]\n",
      " [28717.12 ]\n",
      " [28692.744]\n",
      " [28873.129]\n",
      " [28785.246]\n",
      " [28928.533]\n",
      " [28922.176]\n",
      " [29029.377]\n",
      " [29028.506]\n",
      " [28960.525]]\n"
     ]
    }
   ],
   "source": [
    "predictions_3=model.predict(X_scaled_test)\n",
    "print(predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "658b79d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 172599.629\n"
     ]
    }
   ],
   "source": [
    "#Finally, we use the trained model to predict the testing data and display metrics:\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "print(f'MSE: {mean_squared_error(y_test,predictions_3):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dae5eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 371.248\n"
     ]
    }
   ],
   "source": [
    "print(f'MAE: {mean_absolute_error(y_test,predictions_3):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5be0206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.851\n"
     ]
    }
   ],
   "source": [
    "print(f'R^2: {r2_score(y_test, predictions_3):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd76128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tuning the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bad01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf390c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_HIDDEN = hp.HParam('hidden_size', hp.Discrete([64, 32,16])) \n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([300, 1000])) \n",
    "HP_LEARNING_RATE= hp.HParam('learning_rate', hp.RealInterval (0.01, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e233e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams, logdir):\n",
    "\n",
    "    model = Sequential([Dense(units=hparams[HP_HIDDEN], activation='relu'),Dense(units=1)])\n",
    "\n",
    "    model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(hparams[HP_LEARNING_RATE]),metrics=['mean_squared_error'])\n",
    "\n",
    "    model.fit(X_scaled_train, y_train,validation_data=(X_scaled_test, y_test), epochs=hparams [HP_EPOCHS], verbose=False, callbacks=[tf.keras.callbacks. TensorBoard (logdir), hp.KerasCallback(logdir, hparams), tf.keras.callbacks. EarlyStopping( monitor= 'val_loss', min_delta=0, patience=200, verbose=0, mode= 'auto',)],)\n",
    "    mse=model.evaluate(X_scaled_test, y_test)[1]\n",
    "\n",
    "\n",
    "    pred = model.predict(X_scaled_test)\n",
    "\n",
    "    r2 = r2_score (y_test, pred)\n",
    "\n",
    "    return mse, r2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cb9143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run (hparams, logdir):\n",
    "\n",
    "    with tf.summary.create_file_writer (logdir).as_default(): \n",
    "        hp.hparams_config(hparams=[HP_HIDDEN, HP_EPOCHS, HP_LEARNING_RATE],\n",
    "                          metrics=[hp.Metric('mean_squared_error', display_name='mse'),\n",
    "                                   hp.Metric('r2', display_name='r2')])\n",
    "        mse, r2 = train_test_model(hparams, logdir)\n",
    "        tf.summary.scalar('mean_squared_error', mse, step=1)\n",
    "        tf.summary.scalar('r2', r2, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eca379b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Starting trial: run-1\n",
      "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.01}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 41098.8203 - mean_squared_error: 41098.8203\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-2\n",
      "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.11}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 86409.5234 - mean_squared_error: 86409.5234\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-3\n",
      "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.21}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 95145.6250 - mean_squared_error: 95145.6250\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-4\n",
      "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.3}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 78686.4062 - mean_squared_error: 78686.4062\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-5\n",
      "{'hidden_size': 16, 'epochs': 300, 'learning_rate': 0.4}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 93800.2031 - mean_squared_error: 93800.2031\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-6\n",
      "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.01}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 109193.3281 - mean_squared_error: 109193.3281\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-7\n",
      "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.11}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 95482.5391 - mean_squared_error: 95482.5391\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-8\n",
      "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.21}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 43308.4492 - mean_squared_error: 43308.4492\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-9\n",
      "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.3}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 43071.8281 - mean_squared_error: 43071.8281\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-10\n",
      "{'hidden_size': 16, 'epochs': 1000, 'learning_rate': 0.4}\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 336849.2812 - mean_squared_error: 336849.2812\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-11\n",
      "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.01}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 58701.6562 - mean_squared_error: 58701.6562\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-12\n",
      "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.11}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 91243.3672 - mean_squared_error: 91243.3672\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-13\n",
      "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.21}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 107661.3906 - mean_squared_error: 107661.3906\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-14\n",
      "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.3}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 94093.3750 - mean_squared_error: 94093.3750\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-15\n",
      "{'hidden_size': 32, 'epochs': 300, 'learning_rate': 0.4}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 92729.2812 - mean_squared_error: 92729.2812\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-16\n",
      "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.01}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 160095.9844 - mean_squared_error: 160095.9844\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-17\n",
      "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.11}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 112304.7969 - mean_squared_error: 112304.7969\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-18\n",
      "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.21}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 29861.2559 - mean_squared_error: 29861.2559\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-19\n",
      "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.3}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 44216.0508 - mean_squared_error: 44216.0508\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-20\n",
      "{'hidden_size': 32, 'epochs': 1000, 'learning_rate': 0.4}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 142789.4375 - mean_squared_error: 142789.4375\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-21\n",
      "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.01}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 46947.0469 - mean_squared_error: 46947.0469\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-22\n",
      "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.11}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 101110.6797 - mean_squared_error: 101110.6797\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-23\n",
      "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.21}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 102292.9688 - mean_squared_error: 102292.9688\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-24\n",
      "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.3}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 77940.8828 - mean_squared_error: 77940.8828\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-25\n",
      "{'hidden_size': 64, 'epochs': 300, 'learning_rate': 0.4}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 53985.8203 - mean_squared_error: 53985.8203\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-26\n",
      "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.01}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 71343.0469 - mean_squared_error: 71343.0469\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-27\n",
      "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.11}\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 54278.4961 - mean_squared_error: 54278.4961\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-28\n",
      "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.21}\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 41124.6914 - mean_squared_error: 41124.6914\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-29\n",
      "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.3}\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 43981.1016 - mean_squared_error: 43981.1016\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "-- Starting trial: run-30\n",
      "{'hidden_size': 64, 'epochs': 1000, 'learning_rate': 0.4}\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 142870.6562 - mean_squared_error: 142870.6562\n",
      "8/8 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "session_num=1\n",
    "for hidden in HP_HIDDEN.domain.values:\n",
    "    for epochs in HP_EPOCHS.domain.values:\n",
    "        for learning_rate in tf.linspace(HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value, 5): \n",
    "                hparams ={HP_HIDDEN: hidden,HP_EPOCHS: epochs,HP_LEARNING_RATE:float(\"%.2f\"%float (learning_rate)),}\n",
    "\n",
    "                run_name = \"run-%d\" % session_num \n",
    "                print('-- Starting trial: %s' % run_name) \n",
    "                print({h.name: hparams[h] for h in hparams}) \n",
    "                run(hparams, 'logs/hparam_tuning/' + run_name) \n",
    "                session_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc22ff0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13570), started 0:08:49 ago. (Use '!kill 13570' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ee4126b3d2b4e6ac\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ee4126b3d2b4e6ac\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4f884ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /content/logs/hparam_tuning: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls /content/logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f189520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#we use the optimal model to make predictions:\n",
    "model=Sequential([Dense(units=16, activation='relu'), Dense(units=1)])\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.21))\n",
    "model.fit(X_scaled_train, y_train, epochs=1000, verbose=False)\n",
    "predictions_4= model.predict(X_scaled_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be301270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the prediction along with the ground truth as follows:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(data_test.index, y_test, c='k')\n",
    "plt.plot(data_test.index,predictions_4,c='b')\n",
    "plt.plot(data_test.index,predictions_4,c='r')\n",
    "plt.plot(data_test.index,predictions_4,c='g')\n",
    "plt.xticks(range(0,252,10), rotation=60)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close price')\n",
    "plt.legend(['Truth', 'Neural network prediction'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596cae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
